



<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Explaining quantitative measures of fairness &mdash; SHAP latest documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=7ab3649f" />
      <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css?v=2aa19091" />
      <link rel="stylesheet" href="../../_static/css/style.css" type="text/css" />

  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=c6e86fd7"></script>
      <script src="../../_static/doctools.js?v=9a2dae69"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Tabular examples" href="../../tabular_examples.html" />
    <link rel="prev" title="Be careful when interpreting predictive models in search of causal insights" href="Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%20insights.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #343131" >

          
          
          <a href="../../index.html">
            
              <img src="../../_static/shap_logo_white.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../overviews.html">Topical overviews</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html">An introduction to explainable AI with Shapley values</a></li>
<li class="toctree-l2"><a class="reference internal" href="Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%20insights.html">Be careful when interpreting predictive models in search of causal insights</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Explaining quantitative measures of fairness</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#How-SHAP-can-be-used-to-explain-various-measures-of-model-fairness">How SHAP can be used to explain various measures of model fairness</a></li>
<li class="toctree-l3"><a class="reference internal" href="#What-SHAP-fairness-explanations-look-like-in-various-simulated-scenarios">What SHAP fairness explanations look like in various simulated scenarios</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Scenario-A:-No-reporting-errors">Scenario A: No reporting errors</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Scenario-B:-An-under-reporting-bias-for-women's-income">Scenario B: An under-reporting bias for women’s income</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Scenario-C:-An-under-reporting-bias-for-women's-late-payments">Scenario C: An under-reporting bias for women’s late payments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Scenario-D:-An-under-reporting-bias-for-women's-default-rates">Scenario D: An under-reporting bias for women’s default rates</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Scenario-E:-An-under-reporting-bias-for-women's-default-rates,-take-2">Scenario E: An under-reporting bias for women’s default rates, take 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Scenario-F:-Teasing-apart-multiple-under-reporting-biases">Scenario F: Teasing apart multiple under-reporting biases</a></li>
<li class="toctree-l3"><a class="reference internal" href="#How-introducing-a-protected-feature-can-help-distinguish-between-label-bias-and-feature-bias">How introducing a protected feature can help distinguish between label bias and feature bias</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tabular_examples.html">Tabular examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../text_examples.html">Text examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../image_examples.html">Image examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../genomic_examples.html">Genomic examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_examples.html">API examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../benchmarks.html">Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../release_notes.html">Release notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing.html">Contributing guide</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #343131" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">SHAP</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../overviews.html">Topical Overviews</a></li>
      <li class="breadcrumb-item active">Explaining quantitative measures of fairness</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/example_notebooks/overviews/Explaining quantitative measures of fairness.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Explaining-quantitative-measures-of-fairness">
<h1>Explaining quantitative measures of fairness<a class="headerlink" href="#Explaining-quantitative-measures-of-fairness" title="Link to this heading"></a></h1>
<p>This hands-on article connects explainable AI methods with fairness measures and shows how modern explainability methods can enhance the usefulness of quantitative fairness metrics. By using <a class="reference external" href="http://github.com/shap/shap">SHAP</a> (a popular explainable AI tool) we can decompose measures of fairness and allocate responsibility for any observed disparity among each of the model’s input features. Explaining these quantitative fairness metrics can reduce the concerning tendency to rely on them as
opaque standards of fairness, and instead promote their informed use as tools for understanding how model behavior differs between groups.</p>
<p>Quantitative fairness metrics seek to bring mathematical precision to the definition of fairness in machine learning [<a class="reference external" href="https://books.google.com/books/about/The_Ethical_Algorithm.html?id=QmmtDwAAQBAJ&amp;source=kp_book_description">1</a>]. Definitions of fairness however are deeply rooted in human ethical principles, and so on value judgements that often depend critically on the context in which a machine learning model is being used. This practical dependence on value judgements manifests itself in
the mathematics of quantitative fairness measures as a set of trade-offs between sometimes mutually incompatible definitions of fairness [<a class="reference external" href="https://arxiv.org/abs/1609.05807">2</a>]. Since fairness relies on context-dependent value judgements it is dangerous to treat quantitative fairness metrics as opaque black-box measures of fairness, since doing so may obscure important value judgment choices.</p>
<!--This article covers:

1. How SHAP can be used to explain various measures of model fairness.
2. What SHAP fairness explanations look like in various simulated scenarios.
3. How introducing a protected feature can help distiguish between label bias vs. feature bias.
4. Things you can't learn from a SHAP fairness explanation.--><section id="How-SHAP-can-be-used-to-explain-various-measures-of-model-fairness">
<h2>How SHAP can be used to explain various measures of model fairness<a class="headerlink" href="#How-SHAP-can-be-used-to-explain-various-measures-of-model-fairness" title="Link to this heading"></a></h2>
<p>This article is not about how to choose the “correct” measure of model fairness, but rather about explaining whichever metric you have chosen. Which fairness metric is most appropriate depends on the specifics of your context, such as what laws apply, how the output of the machine learning model impacts people, and what value you place on various outcomes and hence tradeoffs. Here we will use the classic <a class="reference external" href="https://fairmlbook.org/classification.html">demographic parity</a> metric, since it is
simple and closely connected to the legal notion of disparate impact. The same analysis can also be applied to other metrics such as <a class="reference external" href="https://arxiv.org/abs/1808.00023">decision theory cost</a>, <a class="reference external" href="https://arxiv.org/pdf/1803.02453.pdf">equalized odds</a>, <a class="reference external" href="https://ttic.uchicago.edu/~nati/Publications/HardtPriceSrebro2016.pdf">equal opportunity</a>, or <a class="reference external" href="https://github.com/fairlearn/fairlearn/blob/master/TERMINOLOGY.md">equal quality of service</a>. Demographic parity states that the output of the
machine learning model should be equal between two or more groups. The demographic parity difference is then a measure of how much disparity there is between model outcomes in two groups of samples.</p>
<p><strong>Since SHAP decomposes the model output into feature attributions with the same units as the original model output, we can first decompose the model output among each of the input features using SHAP, and then compute the demographic parity difference (or any other fairness metric) for each input feature seperately using the SHAP value for that feature.</strong> Because the SHAP values sum up to the model’s output, the sum of the demographic parity differences of the SHAP values also sum up to the
demographic parity difference of the whole model.</p>
<!--To  will not explain

The danger of treating quantitative fairness metrics as opaque, black-box measures of fairness is strikingly similar to a related problem of treating machine learning models themselves as opaque, black-box predictors. While using a black-box is reasonable in many cases, important problems and assumptions can often be hidden (and hence ignored) when users don't understand the reasons behind a model's behavior \cite{ribeiro2016should}. In response to this problem many explainable AI methods have been developed to help users understand the behavior of modern complex models \cite{vstrumbelj2014explaining,ribeiro2016should,lundberg2017unified}. Here we explore how to apply explainable AI methods to quantitative fairness metrics.--></section>
<section id="What-SHAP-fairness-explanations-look-like-in-various-simulated-scenarios">
<h2>What SHAP fairness explanations look like in various simulated scenarios<a class="headerlink" href="#What-SHAP-fairness-explanations-look-like-in-various-simulated-scenarios" title="Link to this heading"></a></h2>
<p>To help us explore the potential usefulness of explaining quantitative fairness metrics we consider a simple simulated scenario based on credit underwriting. In our simulation there are four underlying factors that drive the risk of default for a loan: income stability, income amount, spending restraint, and consistency. These underlying factors are not observed, but they variously influence four different observable features: job history, reported income, credit inquiries, and late payments.
Using this simulation we generate random samples and then train a non-linear <a class="reference external" href="https://xgboost.ai/">XGBoost</a> classifier to predict the probability of default. The same process also works for any other model type supported by SHAP, just remember that explanations of more complicated models hide more of the model’s details.</p>
<p>By introducing sex-specific reporting errors into a fully specified simulation we can observe how the biases caused by these errors are captured by our chosen fairness metric. In our simulated case the true labels (will default on a loan) are statistically independent of sex (the sensitive class we use to check for fairness). So any disparity between men and women means one or both groups are being modeled incorrectly due to feature measurement errors, labeling errors, or model errors. If the
true labels you are predicting (which might be different than the training labels you have access to) are not statistically independent of the sensitive feature you are considering, then even a perfect model with no errors would fail demographic parity. In these cases fairness explanations can help you determine which sources of demographic disparity are valid.</p>
<!--This article explores how we can use modern explainable AI tools to enhance traditional quantitative measures of model fairness. It is practical and hands-on, so feel free to follow along in the associated [notebook]. I assume you have a basic understanding of how people measure fairness for machine learning models. If you have never before considered fairness in the context of machine learning, then I recommend starting with a basic introduction such as XXX. I am not writing this Here I do not  beforeIt is not meant to be a definitite  One futher disclaimer is that as the author of SHAP (a popular explainable AI tool) I am very familar with the strengths and weaknesses of explainable AI tools, but I do not consider myself a fairness expert. So consider this a thought-provoking guide on how explainable AI tools can enhance quantitative measures of model fairness

I consider myself fairly well informed about explainable AI, but I

Questions about fairness and equal treatment naturally arise whenever the outputs of a machine learning model impact people. For sensitive use-cases such as credit underwriting or crime prediction there are even laws that govern certain aspects of fairness. While fairness issues are not new, the rising popularily of machine learning model

Legal fairness protections are even legally encorced for sensitive use-cases such as credit underwriting or crime prediction, but is also important in many other situations such as quality of service, or you might not initially to consider whenever you are using m Quantifying the fairness of a machine learning model has recently received considerable attention in the research community, and many quantitative fairness metrics have been proposed. In parallel to this work on fairness, explaining the outputs of a machine learning model has also received considerable research attention. %Explainability is intricately connected to fairness, since good explanations enable users to understand a model's behavior and so judge its fairness.

Here we connect explainability methods with fairness measures and show how recent explainability methods can enhance the usefulness of quantitative fairness metrics by decomposing them among the model's input features. Explaining quantitative fairness metrics can reduce our tendency to rely on them as opaque standards of fairness, and instead promote their informed use as tools for understanding model behavior between groups.

This notebook explores how SHAP can be used to explain quantitative measures of fairness, and so enhance their usefulness. To do this we consider a simple simulated scenario based on credit underwriting. In the simulation below there are four underlying factors that drive the risk of default for a loan: income stability, income amount, spending restraint, and consistency. These underlying factors are not observed, but they influence four different observable features in various ways: job history, reported income, credit inquiries, and late payments. Using this simulation we generate random samples and then train a non-linear gradient boosting tree classifier to predict the probability of default.

By introducing sex-specific reporting errors into the simulation we can observe how the biases caused by these errors are captured by fairness metrics. For this analysis we use the classic statistical parity metric, though the same analysis works with other metrics. Note that for a more detailed description of fairness metrics you can check out the [fairlearn package's documentation](https://github.com/fairlearn/fairlearn/blob/master/TERMINOLOGY.md#fairness-of-ai-systems).--><div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># here we define a function that we can call to execute our simulation under</span>
<span class="c1"># a variety of different alternative scenarios</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">shap</span>

<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = &#39;retina&#39;


<span class="k">def</span> <span class="nf">run_credit_experiment</span><span class="p">(</span>
    <span class="n">N</span><span class="p">,</span>
    <span class="n">job_history_sex_impact</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">reported_income_sex_impact</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">income_sex_impact</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">late_payments_sex_impact</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">default_rate_sex_impact</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">include_brandx_purchase_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">include_sex</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sex</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>  <span class="c1"># randomly half men and half women</span>

    <span class="c1"># four hypothetical causal factors influence customer quality</span>
    <span class="c1"># they are all scaled to the same units between 0-1</span>
    <span class="n">income_stability</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">income_amount</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">income_sex_impact</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">income_amount</span> <span class="o">-=</span> <span class="n">income_sex_impact</span> <span class="o">/</span> <span class="mi">90000</span> <span class="o">*</span> <span class="n">sex</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
        <span class="n">income_amount</span> <span class="o">-=</span> <span class="n">income_amount</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
        <span class="n">income_amount</span> <span class="o">/=</span> <span class="n">income_amount</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">spending_restraint</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">consistency</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># intuitively this product says that high customer quality comes from simultaneously</span>
    <span class="c1"># being strong in all factors</span>
    <span class="n">customer_quality</span> <span class="o">=</span> <span class="n">income_stability</span> <span class="o">*</span> <span class="n">income_amount</span> <span class="o">*</span> <span class="n">spending_restraint</span> <span class="o">*</span> <span class="n">consistency</span>

    <span class="c1"># job history is a random function of the underlying income stability feature</span>
    <span class="n">job_history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
        <span class="mi">10</span> <span class="o">*</span> <span class="n">income_stability</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">-</span> <span class="n">job_history_sex_impact</span> <span class="o">*</span> <span class="n">sex</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
        <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># reported income is a random function of the underlying income amount feature</span>
    <span class="n">reported_income</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
        <span class="mi">10000</span>
        <span class="o">+</span> <span class="mi">90000</span> <span class="o">*</span> <span class="n">income_amount</span>
        <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10000</span>
        <span class="o">-</span> <span class="n">reported_income_sex_impact</span> <span class="o">*</span> <span class="n">sex</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">),</span>
        <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># credit inquiries is a random function of the underlying spending restraint and income amount features</span>
    <span class="n">credit_inquiries</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="o">-</span><span class="n">spending_restraint</span> <span class="o">+</span> <span class="n">income_amount</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.1</span>
    <span class="p">)</span>

    <span class="c1"># credit inquiries is a random function of the underlying consistency and income stability features</span>
    <span class="n">late_payments</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">consistency</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">income_stability</span><span class="p">),</span> <span class="mi">0</span><span class="p">))</span>
        <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.1</span><span class="p">)</span>
        <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">late_payments_sex_impact</span> <span class="o">*</span> <span class="n">sex</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)),</span>
        <span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># bundle everything into a data frame and define the labels based on the default rate and customer quality</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s2">&quot;Job history&quot;</span><span class="p">:</span> <span class="n">job_history</span><span class="p">,</span>
            <span class="s2">&quot;Reported income&quot;</span><span class="p">:</span> <span class="n">reported_income</span><span class="p">,</span>
            <span class="s2">&quot;Credit inquiries&quot;</span><span class="p">:</span> <span class="n">credit_inquiries</span><span class="p">,</span>
            <span class="s2">&quot;Late payments&quot;</span><span class="p">:</span> <span class="n">late_payments</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="n">default_rate</span> <span class="o">=</span> <span class="mf">0.40</span> <span class="o">+</span> <span class="n">sex</span> <span class="o">*</span> <span class="n">default_rate_sex_impact</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">customer_quality</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">customer_quality</span><span class="p">,</span> <span class="n">default_rate</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">include_brandx_purchase_score</span><span class="p">:</span>
        <span class="n">brandx_purchase_score</span> <span class="o">=</span> <span class="n">sex</span> <span class="o">+</span> <span class="mf">0.8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
        <span class="n">X</span><span class="p">[</span><span class="s2">&quot;Brand X purchase score&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">brandx_purchase_score</span>

    <span class="k">if</span> <span class="n">include_sex</span><span class="p">:</span>
        <span class="n">X</span><span class="p">[</span><span class="s2">&quot;Sex&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">sex</span> <span class="o">+</span> <span class="mi">0</span>

    <span class="c1"># build model</span>
    <span class="kn">import</span> <span class="nn">xgboost</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">xgboost</span><span class="o">.</span><span class="n">XGBClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># build explanation</span>
    <span class="kn">import</span> <span class="nn">shap</span>

    <span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">shap</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>
    <span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">shap_values</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">shap_values</span><span class="p">,</span> <span class="n">sex</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">explainer</span><span class="o">.</span><span class="n">expected_value</span>
</pre></div>
</div>
</div>
<!--## Scenario A: No reporting errors

As a baseline experiment we refrain from introducing any sex-specific reporting errors. This results in no significant statistical parity difference between the credit score of men and women:--></section>
<section id="Scenario-A:-No-reporting-errors">
<h2>Scenario A: No reporting errors<a class="headerlink" href="#Scenario-A:-No-reporting-errors" title="Link to this heading"></a></h2>
<p>Our first experiment is a simple baseline check where we refrain from introducing any sex-specific reporting errors. While we could use any model output to measure demographic parity, we use the continuous log-odds score from a binary XGBoost classifier. As expected, this baseline experiment results in no significant demographic parity difference between the credit scores of men and women. We can see this by plotting the difference between the average credit score for women and men as a bar plot
and noting that zero is close to the margin of error (note that negative values mean women have a lower average predicted risk than men, and positive values mean that women have a higher average predicted risk than men):</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">shap_values_A</span><span class="p">,</span> <span class="n">sex_A</span><span class="p">,</span> <span class="n">X_A</span><span class="p">,</span> <span class="n">ev_A</span> <span class="o">=</span> <span class="n">run_credit_experiment</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">model_outputs_A</span> <span class="o">=</span> <span class="n">ev_A</span> <span class="o">+</span> <span class="n">shap_values_A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">glabel</span> <span class="o">=</span> <span class="s2">&quot;Demographic parity difference</span><span class="se">\n</span><span class="s2">of model output for women vs. men&quot;</span>
<span class="n">xmin</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.8</span>
<span class="n">xmax</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">sex_A</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">glabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_3_0.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_3_0.png" style="width: 393px; height: 123px;" />
</div>
</div>
<p>Now we can use SHAP to decompose the model output among each of the model’s input features and then compute the demographic parity difference on the component attributed to each feature. As noted above, because the SHAP values sum up to the model’s output, the sum of the demographic parity differences of the SHAP values for each feature sum up to the demographic parity difference of the whole model. This means that the sum of the bars below equals the bar above (the demographic parity difference
of our baseline scenario model).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">slabel</span> <span class="o">=</span> <span class="s2">&quot;Demographic parity difference</span><span class="se">\n</span><span class="s2">of SHAP values for women vs. men&quot;</span>
<span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_A</span><span class="p">,</span> <span class="n">sex_A</span><span class="p">,</span> <span class="n">X_A</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">slabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_5_0.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_5_0.png" style="width: 499px; height: 269px;" />
</div>
</div>
</section>
<section id="Scenario-B:-An-under-reporting-bias-for-women's-income">
<h2>Scenario B: An under-reporting bias for women’s income<a class="headerlink" href="#Scenario-B:-An-under-reporting-bias-for-women's-income" title="Link to this heading"></a></h2>
<p>In our baseline scenario we designed a simulation where sex had no impact on any of the features or labels used by the model. Here in scenario B we introduce an under-reporting bias for women’s income into the simulation. The point here is not how realistic it would be for women’s income to be under-reported in the real-world, but rather how we can identify that a sex-specific bias has been introduced and understand where it came from. By plotting the difference in average model output (default
risk) between women and men we can see that the income under-reporting bias has created a significant demographic parity difference where women now have a higher risk of default than men:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap_values_B</span><span class="p">,</span> <span class="n">sex_B</span><span class="p">,</span> <span class="n">X_B</span><span class="p">,</span> <span class="n">ev_B</span> <span class="o">=</span> <span class="n">run_credit_experiment</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">reported_income_sex_impact</span><span class="o">=</span><span class="mi">30000</span><span class="p">)</span>
<span class="n">model_outputs_B</span> <span class="o">=</span> <span class="n">ev_B</span> <span class="o">+</span> <span class="n">shap_values_B</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_B</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">sex_B</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">glabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
 95%|=================== | 9542/10000 [00:11&lt;00:00]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_7_1.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_7_1.png" style="width: 393px; height: 123px;" />
</div>
</div>
<p>If this were a real application, this demographic parity difference might trigger an in-depth analysis of the model to determine what might be causing the disparity. While this investigation is challenging given just a single demographic parity difference value, it is much easier given the per-feature demographic parity decomposition based on SHAP. Using SHAP we can see there is a significant bias coming from the reported income feature that is increasing the risk of women disproportionately to
men. This allows us to quickly identify which feature has the reporting bias that is causing our model to violate demographic parity:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_B</span><span class="p">,</span> <span class="n">sex_B</span><span class="p">,</span> <span class="n">X_B</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">slabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_9_0.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_9_0.png" style="width: 499px; height: 269px;" />
</div>
</div>
<p>It is important to note at this point how our assumptions can impact the interpretation of SHAP fairness explanations. In our simulated scenario we know that women actually have identical income profiles to men, so when we see that the reported income feature is biased lower for women than for men, we know that has come from a bias in the measurement errors in the reported income feature. The best way to address this problem would be figure out how to debias the measurement errors in the
reported income feature. Doing so would create a more accurate model that also has less demographic disparity. However, if we instead assume that women actually are making less money than men (and it is not just a reporting error), then we can’t just “fix” the reported income feature. Instead we have to carefully consider how best to account for real differences in default risk between two protected groups. It is impossible to determine which of these two situations is happening using just the
SHAP fairness explanation, since in both cases the reported income feature will be responsible for an observed disparity between the predicted risks of men and women.</p>
</section>
<section id="Scenario-C:-An-under-reporting-bias-for-women's-late-payments">
<h2>Scenario C: An under-reporting bias for women’s late payments<a class="headerlink" href="#Scenario-C:-An-under-reporting-bias-for-women's-late-payments" title="Link to this heading"></a></h2>
<p>To verify that SHAP demographic parity explanations can correctly detect disparities regardless of the direction of effect or source feature, we repeat our previous experiment but instead of an under-reporting bias for income, we introduce an under-reporting bias for women’s late payment rates. This results in a significant demographic parity difference for the model’s output where now women have a lower average default risk than men:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap_values_C</span><span class="p">,</span> <span class="n">sex_C</span><span class="p">,</span> <span class="n">X_C</span><span class="p">,</span> <span class="n">ev_C</span> <span class="o">=</span> <span class="n">run_credit_experiment</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">late_payments_sex_impact</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model_outputs_C</span> <span class="o">=</span> <span class="n">ev_C</span> <span class="o">+</span> <span class="n">shap_values_C</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_C</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">sex_C</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">glabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_12_0.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_12_0.png" style="width: 393px; height: 123px;" />
</div>
</div>
<p>And as we would hope, the SHAP explanations correctly highlight the late payments feature as the cause of the model’s demographic parity difference, as well as the direction of the effect:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_C</span><span class="p">,</span> <span class="n">sex_C</span><span class="p">,</span> <span class="n">X_C</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">slabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_14_0.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_14_0.png" style="width: 499px; height: 269px;" />
</div>
</div>
</section>
<section id="Scenario-D:-An-under-reporting-bias-for-women's-default-rates">
<h2>Scenario D: An under-reporting bias for women’s default rates<a class="headerlink" href="#Scenario-D:-An-under-reporting-bias-for-women's-default-rates" title="Link to this heading"></a></h2>
<p>The experiments above focused on introducing reporting errors for specific input features. Next we consider what happens when we introduce reporting errors on the training labels through an under-reporting bias on women’s default rates (which means defaults are less likely to be reported for women than men). Interestingly, for our simulated scenario this results in no significant demographic parity differences in the model’s output:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap_values_D</span><span class="p">,</span> <span class="n">sex_D</span><span class="p">,</span> <span class="n">X_D</span><span class="p">,</span> <span class="n">ev_D</span> <span class="o">=</span> <span class="n">run_credit_experiment</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">default_rate_sex_impact</span><span class="o">=-</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># 20% change</span>
<span class="n">model_outputs_D</span> <span class="o">=</span> <span class="n">ev_D</span> <span class="o">+</span> <span class="n">shap_values_D</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_D</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">sex_D</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">glabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_16_0.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_16_0.png" style="width: 393px; height: 123px;" />
</div>
</div>
<p>We also see no evidence of any demographic parity differences in the SHAP explanations:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_D</span><span class="p">,</span> <span class="n">sex_D</span><span class="p">,</span> <span class="n">X_D</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">slabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_18_0.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_18_0.png" style="width: 499px; height: 269px;" />
</div>
</div>
</section>
<section id="Scenario-E:-An-under-reporting-bias-for-women's-default-rates,-take-2">
<h2>Scenario E: An under-reporting bias for women’s default rates, take 2<a class="headerlink" href="#Scenario-E:-An-under-reporting-bias-for-women's-default-rates,-take-2" title="Link to this heading"></a></h2>
<p>It may at first be surprising that no demographic parity differences were caused when we introduced an under-reporting bias on women’s default rates. This is because none of the four features in our simulation are significantly correlated with sex, so none of them could be effectively used to model the bias we introduced into the training labels. If we now instead provide a new feature (brand X purchase score) to the model that is correlated with sex, then we see a demographic parity difference
emerge as that feature is used by the model to capture the sex-specific bias in the training labels:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap_values_E</span><span class="p">,</span> <span class="n">sex_E</span><span class="p">,</span> <span class="n">X_E</span><span class="p">,</span> <span class="n">ev_E</span> <span class="o">=</span> <span class="n">run_credit_experiment</span><span class="p">(</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">default_rate_sex_impact</span><span class="o">=-</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">include_brandx_purchase_score</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="n">model_outputs_E</span> <span class="o">=</span> <span class="n">ev_E</span> <span class="o">+</span> <span class="n">shap_values_E</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_E</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">sex_E</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">glabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
 98%|===================| 9794/10000 [00:11&lt;00:00]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_20_1.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_20_1.png" style="width: 393px; height: 123px;" />
</div>
</div>
<p>When we explain the demographic parity difference with SHAP we see that, as expected, the brand X purchase score feature drives the difference. In this case it is not because we have a bias in how we measure the brand X purchase score feature, but rather because we have a bias in our training label that gets captured by any input features that are sufficiently correlated with sex:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_E</span><span class="p">,</span> <span class="n">sex_E</span><span class="p">,</span> <span class="n">X_E</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">slabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_22_0.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_22_0.png" style="width: 542px; height: 318px;" />
</div>
</div>
</section>
<section id="Scenario-F:-Teasing-apart-multiple-under-reporting-biases">
<h2>Scenario F: Teasing apart multiple under-reporting biases<a class="headerlink" href="#Scenario-F:-Teasing-apart-multiple-under-reporting-biases" title="Link to this heading"></a></h2>
<p>When there is a single cause of reporting bias then both the classic demographic parity test on the model’s output, and the SHAP explanation of the demographic parity test capture the same bias effect (though the SHAP explanation can often have more statistical significance since it isolates the feature causing the bias). But what happens when there are multiple causes of bias occurring in a dataset? In this experiment we introduce two such biases, an under-reporting of women’s default rates,
and an under-reporting of women’s job history. These biases tend to offset each other in the global average and so a demographic parity test on the model’s output shows no measurable disparity:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap_values_F</span><span class="p">,</span> <span class="n">sex_F</span><span class="p">,</span> <span class="n">X_F</span><span class="p">,</span> <span class="n">ev_F</span> <span class="o">=</span> <span class="n">run_credit_experiment</span><span class="p">(</span>
    <span class="n">N</span><span class="p">,</span>
    <span class="n">default_rate_sex_impact</span><span class="o">=-</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">include_brandx_purchase_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">job_history_sex_impact</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model_outputs_F</span> <span class="o">=</span> <span class="n">ev_F</span> <span class="o">+</span> <span class="n">shap_values_F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">sex_F</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">glabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
100%|===================| 9996/10000 [00:11&lt;00:00]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_24_1.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_24_1.png" style="width: 393px; height: 123px;" />
</div>
</div>
<p>However, if we look at the SHAP explanation of the demographic parity difference we clearly see both (counteracting) biases:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_F</span><span class="p">,</span> <span class="n">sex_F</span><span class="p">,</span> <span class="n">X_F</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">slabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_26_0.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_26_0.png" style="width: 542px; height: 318px;" />
</div>
</div>
<p>Identifying multiple potentially offsetting bias effects can be important since while on average there is no disparate impact on men or women, there is disparate impact on individuals. For example, in this simulation women who have not shopped at brand X will receive a lower credit score than they should have because of the bias present in job history reporting.</p>
</section>
<section id="How-introducing-a-protected-feature-can-help-distinguish-between-label-bias-and-feature-bias">
<h2>How introducing a protected feature can help distinguish between label bias and feature bias<a class="headerlink" href="#How-introducing-a-protected-feature-can-help-distinguish-between-label-bias-and-feature-bias" title="Link to this heading"></a></h2>
<p>In scenario F we were able to pick apart two distict forms of bias, one coming from job history under-reporting and one coming from default rate under-reporting. However, the bias from default rate under-reporting was not attributed to the default rate label, but rather to the brand X purchase score feature that happened to be correlated with sex. This still leaves us with some uncertainty about the true sources of demographic parity differences, since any difference attributed to an input
feature could be due to an issue with that feature, or due to an issue with the training labels.</p>
<p>It turns out that in this case we can help disentangle label bias from feature bias by introducing sex as a variable directly into the model. The goal of introducing sex as an input feature is to cause the label bias to fall entirely on the sex feature, leaving the feature biases untouched. So we can then distinguish between label biases and feature biases by comparing the results of scenario F above to our new scenario G below. This of course creates an even stronger demographic parity
difference than we had before, but that is fine since our goal here is not bias mitigation, but rather bias understanding.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap_values_G</span><span class="p">,</span> <span class="n">sex_G</span><span class="p">,</span> <span class="n">X_G</span><span class="p">,</span> <span class="n">ev_G</span> <span class="o">=</span> <span class="n">run_credit_experiment</span><span class="p">(</span>
    <span class="n">N</span><span class="p">,</span>
    <span class="n">default_rate_sex_impact</span><span class="o">=-</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">include_brandx_purchase_score</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">job_history_sex_impact</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">include_sex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model_outputs_G</span> <span class="o">=</span> <span class="n">ev_G</span> <span class="o">+</span> <span class="n">shap_values_G</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_G</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">sex_G</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">glabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
 97%|=================== | 9720/10000 [00:11&lt;00:00]
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_29_1.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_29_1.png" style="width: 393px; height: 123px;" />
</div>
</div>
<p>The SHAP explanation for scenario G shows that all of the demographic parity difference that used to be attached to the brand X purchase score feature in scenario F has now moved to the sex feature, while none of the demographic parity difference attached to the job history feature in scenario F has moved. This can be interpreted to mean that all of the disparity attributed to brand X purchase score in scenario F was due to label bias, while all of the disparity attributed to job history in
scenario F was due to feature bias.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap</span><span class="o">.</span><span class="n">group_difference_plot</span><span class="p">(</span><span class="n">shap_values_G</span><span class="p">,</span> <span class="n">sex_G</span><span class="p">,</span> <span class="n">X_G</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">xmin</span><span class="o">=</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="o">=</span><span class="n">xmax</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="n">slabel</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_31_0.png" class="no-scaled-link" src="../../_images/example_notebooks_overviews_Explaining_quantitative_measures_of_fairness_31_0.png" style="width: 542px; height: 367px;" />
</div>
</div>
</section>
<section id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Link to this heading"></a></h2>
<!--## Things you can't learn from a SHAP fairness explanation--><p>Fairness is a complex topic where clean mathematical answers almost always come with caveats and depend on ethical value judgements. This means that it is particularly important to not just use fairness metrics as black-boxes, but rather seek to understand how these metrics are computed and what aspects of your model and training data are impacting any disparities you observe. Decomposing quantitative fairness metrics using SHAP can reduce their opacity when the metrics are driven by measurement
biases effecting only a few features. I hope you find the fairness explanations we demonstrated here help you better wrestle with the underlying value judgements inherent in fairness evaluation, and so help reduce the risk of unintended consequences that comes when we use fairness metrics in real world contexts.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%20insights.html" class="btn btn-neutral float-left" title="Be careful when interpreting predictive models in search of causal insights" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../tabular_examples.html" class="btn btn-neutral float-right" title="Tabular examples" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, Scott Lundberg.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>